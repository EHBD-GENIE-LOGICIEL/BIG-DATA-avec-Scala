{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data avec Spark : Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`Nom & Prenom : `*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problematique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce projet consiste à utiliser Apache Spark pour faire l'analyse et le traitement des données de **[San Francisco Fire Department Calls ](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)** afin de fournir quelques KPI (*Key Performance Indicator*). Le **SF Fire Datasets** comprend les réponses aux appels de toutes les unités d'incendie. Chaque enregistrement comprend le numéro d'appel, le numéro d'incident, l'adresse, l'identifiant de l'unité, le type d'appel et la disposition. Tous les intervalles de temps pertinents sont également inclus. Étant donné que ce Dataset est basé sur les réponses et que la plupart des appels impliquent plusieurs unités, ainsi il existe plusieurs enregistrements pour chaque numéro d'appel. Les adresses sont associées à un numéro de bloc, à une intersection ou à une boîte d'appel, et non à une adresse spécifique.\n",
    "\n",
    "**Plus de details sur la description des données [ici](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)**\n",
    "\n",
    "**Download csv file [here](https://data.sfgov.org/api/views/nuek-vuh3/rows.csv?accessType=DOWNLOAD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à faire.\n",
    "L'objectif de ce travail est de comprendre le Dataset SF Fire afin de bien répondre aux questions en utilisant les codes Spark/Scala adéquats.\n",
    "\n",
    "- Code lisible et bien indenté, \n",
    "- N'oublier pas de mettre en commentaire la justification de votre réponse sur les cellule Markdown. \n",
    "\n",
    "\n",
    "#### Note:\n",
    "- Vous pouvez en groupe (au plus deux étudiants) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//creation d un dataframe\n",
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n",
    "import $ivy.`sh.almond::almond-spark:0.10.9`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Importez les modules Spark necessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mrootLogger\u001b[39m: \u001b[32mLogger\u001b[39m = org.apache.log4j.spi.RootLogger@4af6d4df\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val rootLogger = Logger.getRootLogger()\n",
    "rootLogger.setLevel(Level.ERROR)\n",
    "\n",
    "Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)\n",
    "Logger.getLogger(\"org.spark-project\").setLevel(Level.WARN)\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Creez la Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/11 20:46:08 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@7c5752c2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local\")\n",
    "    .appName(\"BD-FS FIRE\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Chargez les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez le `fireSchema` definit dans la cellule suivante pour le chargement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[36mfireSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallNumber\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitID\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"IncidentNumber\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallType\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallDate\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"WatchDate\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallFinalDisposition\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"AvailableDtTm\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"City\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Zipcode\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Battalion\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StationArea\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Box\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"OriginalPriority\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Priority\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"FinalPriority\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"ALSUnit\"\u001b[39m, BooleanType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallTypeGroup\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"NumAlarms\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitType\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitSequenceInCallDispatch\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"FirePreventionDistrict\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"SupervisorDistrict\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Neighborhood\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Location\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"RowID\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Delay\"\u001b[39m, FloatType, true, {})\n",
       ")\n",
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"sf-fire-calls.csv\"\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "  StructField(\"UnitID\", StringType, true),\n",
    "  StructField(\"IncidentNumber\", IntegerType, true),\n",
    "  StructField(\"CallType\", StringType, true),                  \n",
    "  StructField(\"CallDate\", StringType, true),      \n",
    "  StructField(\"WatchDate\", StringType, true),\n",
    "  StructField(\"CallFinalDisposition\", StringType, true),\n",
    "  StructField(\"AvailableDtTm\", StringType, true),\n",
    "  StructField(\"Address\", StringType, true),       \n",
    "  StructField(\"City\", StringType, true),       \n",
    "  StructField(\"Zipcode\", IntegerType, true),       \n",
    "  StructField(\"Battalion\", StringType, true),                 \n",
    "  StructField(\"StationArea\", StringType, true),       \n",
    "  StructField(\"Box\", StringType, true),       \n",
    "  StructField(\"OriginalPriority\", StringType, true),       \n",
    "  StructField(\"Priority\", StringType, true),       \n",
    "  StructField(\"FinalPriority\", IntegerType, true),       \n",
    "  StructField(\"ALSUnit\", BooleanType, true),       \n",
    "  StructField(\"CallTypeGroup\", StringType, true),\n",
    "  StructField(\"NumAlarms\", IntegerType, true),\n",
    "  StructField(\"UnitType\", StringType, true),\n",
    "  StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "  StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "  StructField(\"SupervisorDistrict\", StringType, true),\n",
    "  StructField(\"Neighborhood\", StringType, true),\n",
    "  StructField(\"Location\", StringType, true),\n",
    "  StructField(\"RowID\", StringType, true),\n",
    "  StructField(\"Delay\", FloatType, true)))\n",
    "\n",
    "// your code here (hint spark session name is sparkSession Q2)\n",
    "val path = \"sf-fire-calls.csv\"\n",
    "val data = spark.read\n",
    "          .option(\"header\",\"true\")\n",
    "          .schema(fireSchema)\n",
    "          .csv(path)\n",
    "          //autre methode \n",
    "//spark.read\n",
    "//.format.(\"csv\")\n",
    "//.option(\"delimiter\", \",\").schema(fireSchema).load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le nombre de ligne est 175296\n",
      "le nombre de colonne est 28\n"
     ]
    }
   ],
   "source": [
    "println(\"le nombre de ligne est \" +data.count())\n",
    "println(\"le nombre de colonne est \" +data.columns.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Mettez en cache les donnees chargees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/11 20:46:20 WARN CacheManager: Asked to cache already cached data.\n",
      "23/04/11 20:46:20 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres29_0\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 26 more fields]\n",
       "\u001b[36mres29_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache\n",
    "data.persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.col\u001b[39m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise la mise en cache quand on effectue plusieurs actions sur le même DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Supprimez tous les appels de type `Medical Incident`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: appliquez la methode `.filter()` a la colonne `CallType` avec l'operateur `=!=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_without_mi\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: int, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_without_mi =data.filter(col(\"CallType\")=!= \"Medical Incident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Combien de types d'appels distincts ont été passés ?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.countDistinct\u001b[39m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|Nombre de Type d'appel|\n",
      "+----------------------+\n",
      "|                    30|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.agg(countDistinct(\"CallType\") as \"Nombre de Type d'appel\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Quels types d'appels  ont été passés au service d'incendie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|CallType                                    |\n",
      "+--------------------------------------------+\n",
      "|Elevator / Escalator Rescue                 |\n",
      "|Marine Fire                                 |\n",
      "|Aircraft Emergency                          |\n",
      "|Confined Space / Structure Collapse         |\n",
      "|Administrative                              |\n",
      "|Alarms                                      |\n",
      "|Odor (Strange / Unknown)                    |\n",
      "|Citizen Assist / Service Call               |\n",
      "|HazMat                                      |\n",
      "|Watercraft in Distress                      |\n",
      "|Explosion                                   |\n",
      "|Oil Spill                                   |\n",
      "|Vehicle Fire                                |\n",
      "|Suspicious Package                          |\n",
      "|Extrication / Entrapped (Machinery, Vehicle)|\n",
      "|Other                                       |\n",
      "|Outside Fire                                |\n",
      "|Traffic Collision                           |\n",
      "|Assist Police                               |\n",
      "|Gas Leak (Natural and LP Gases)             |\n",
      "|Water Rescue                                |\n",
      "|Electrical Hazard                           |\n",
      "|High Angle Rescue                           |\n",
      "|Structure Fire                              |\n",
      "|Industrial Accidents                        |\n",
      "|Medical Incident                            |\n",
      "|Mutual Aid / Assist Outside Agency          |\n",
      "|Fuel Spill                                  |\n",
      "|Smoke Investigation (Outside)               |\n",
      "|Train / Rail Incident                       |\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"CallType\").distinct.show(30, truncate =false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. Trouvez toutes les réponses ou les délais sont supérieurs à 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:\n",
    "1. Renommez la colonne `Delay` -> `ReponseDelayedinMins`\n",
    "2. Retournez un nouveau DataFrame\n",
    "3. Affichez tous les appels où le temps de réponse au site d'incendie a eu un retard de plus de 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 26 more fields]\n",
       "\u001b[36mres34_1\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: int, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data1 =data.withColumnRenamed(\"Delay\", \"ReponseDelayedinMins\")\n",
    "data1.filter(col(\"ReponseDelayedinMins\") >5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Convertissez les colonnes dates en timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:\n",
    "* `CallDate` -> `IncidentDate`\n",
    "* `WatchDate` -> `OnWatchDate`\n",
    "* `AvailableDtTm` -> `AvailableDtTS`\n",
    "exemple code pour le cas de `CallDate`:\n",
    "`dataframe.withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data2 =(data.withColumn(\"IncidentDate\",to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\")\n",
    "                 .withColumn(\"OnWatchDate\",to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\")).drop(\"WatchDate\")\n",
    "                 .withColumn(\"AvailableDtTS\",to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy\")).drop(\"AvailableDtTm\"))\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. Quels sont les types d'appels les plus courants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------+\n",
      "|CallType                     |count |\n",
      "+-----------------------------+------+\n",
      "|Medical Incident             |113794|\n",
      "|Structure Fire               |23319 |\n",
      "|Alarms                       |19406 |\n",
      "|Traffic Collision            |7013  |\n",
      "|Citizen Assist / Service Call|2524  |\n",
      "+-----------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.groupBy(\"CallType\")\n",
    ".count()\n",
    ".orderBy(col(\"count\").desc)\n",
    ".show(5, truncate =false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11. Quels sont les boites postales rencontrées dans les appels les plus courants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd37.sc:1: overloaded method value filter with alternatives:\n",
      "  (func: org.apache.spark.api.java.function.FilterFunction[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>\n",
      "  (func: org.apache.spark.sql.Row => Boolean)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>\n",
      "  (conditionExpr: String)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>\n",
      "  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
      " cannot be applied to (Boolean)\n",
      "val df2= data1.filter(col(\"CallType\") == \"Medical Incident\")\n",
      "               ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "val df2= data1.filter(col(\"CallType\") == \"Medical Incident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12. Quels sont les quartiers de San Francisco dont les codes postaux sont `94102` et `94103`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|        Neighborhood|Zipcode|\n",
      "+--------------------+-------+\n",
      "|        Potrero Hill|  94103|\n",
      "|    Western Addition|  94102|\n",
      "|          Tenderloin|  94102|\n",
      "|            Nob Hill|  94102|\n",
      "| Castro/Upper Market|  94103|\n",
      "|     South of Market|  94102|\n",
      "|     South of Market|  94103|\n",
      "|        Hayes Valley|  94103|\n",
      "|Financial Distric...|  94102|\n",
      "|         Mission Bay|  94103|\n",
      "|          Tenderloin|  94103|\n",
      "|Financial Distric...|  94103|\n",
      "|        Hayes Valley|  94102|\n",
      "|             Mission|  94103|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"Neighborhood\", \"Zipcode\")\n",
    ".where(col(\"Zipcode\")=== 94102 || col(\"Zipcode\")=== 94103)\n",
    ".distinct()\n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13. Determinez le nombre total d'appels, ainsi que la moyenne, le minimum et le maximum du temps de réponse des appels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|ReponseDelayedinMins|\n",
      "+-------+--------------------+\n",
      "|  count|              175296|\n",
      "|   mean|   3.892364154521585|\n",
      "| stddev|   9.378286226254257|\n",
      "|    min|         0.016666668|\n",
      "|    max|             1844.55|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.select(\"ReponseDelayedinMins\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14. Combien d'années distinctes trouve t-on dans ce Dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Appliquer la fonction `year()` a la colonne `IncidentDate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`IncidentDate`' given input columns: [Neighborhood, CallType, NumAlarms, AvailableDtTm, RowID, FinalPriority, CallNumber, CallTypeGroup, OriginalPriority, IncidentNumber, ALSUnit, FirePreventionDistrict, Zipcode, CallDate, City, ReponseDelayedinMins, Location, CallFinalDisposition, Box, UnitSequenceInCallDispatch, UnitID, Address, Priority, UnitType, SupervisorDistrict, WatchDate, StationArea, Battalion];;\n'Project [year('IncidentDate) AS year(IncidentDate)#3052]\n+- Project [CallNumber#2101, UnitID#2102, IncidentNumber#2103, CallType#2104, CallDate#2105, WatchDate#2106, CallFinalDisposition#2107, AvailableDtTm#2108, Address#2109, City#2110, Zipcode#2111, Battalion#2112, StationArea#2113, Box#2114, OriginalPriority#2115, Priority#2116, FinalPriority#2117, ALSUnit#2118, CallTypeGroup#2119, NumAlarms#2120, UnitType#2121, UnitSequenceInCallDispatch#2122, FirePreventionDistrict#2123, SupervisorDistrict#2124, ... 4 more fields]\n   +- Relation[CallNumber#2101,UnitID#2102,IncidentNumber#2103,CallType#2104,CallDate#2105,WatchDate#2106,CallFinalDisposition#2107,AvailableDtTm#2108,Address#2109,City#2110,Zipcode#2111,Battalion#2112,StationArea#2113,Box#2114,OriginalPriority#2115,Priority#2116,FinalPriority#2117,ALSUnit#2118,CallTypeGroup#2119,NumAlarms#2120,UnitType#2121,UnitSequenceInCallDispatch#2122,FirePreventionDistrict#2123,SupervisorDistrict#2124,... 4 more fields] csv\n\u001b[39m\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m111\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m108\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m280\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m69\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m280\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m328\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m328\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m69\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m273\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m273\u001b[39m)\n  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m266\u001b[39m)\n  scala.collection.AbstractTraversable.map(\u001b[32mTraversable.scala\u001b[39m:\u001b[32m108\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m108\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m86\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m86\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m83\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m95\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m108\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m201\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m58\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m56\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m48\u001b[39m)\n  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3412\u001b[39m)\n  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1341\u001b[39m)\n  ammonite.$sess.cmd39$Helper.<init>(\u001b[32mcmd39.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd39$.<init>(\u001b[32mcmd39.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd39$.<clinit>(\u001b[32mcmd39.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val df = data1.select(year(col(\"IncidentDate\"))).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q15. Quelle semaine de l'année 2018 a eu le plus d'appels d'incendie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter(year(col(\"IncidentDate\")))===2018 && col(\"CallType\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16. Quels sont les quartiers de San Francisco qui ont connu le pire temps de réponse en 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q17. Stocker les données sous format de fichiers Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").save(\"/tmp/firedataService_parquet/files/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q18. Rechargez  les données stockées en format Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val newdataDF = sparkSession.read.parquet(\"/tmp/firedataService_parquet/files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdataDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
